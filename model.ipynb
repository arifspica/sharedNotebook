{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .boolean { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .integer { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .string  { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Import all the dependencies\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "import pandas as pd\n",
    "from array import array\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.utils import shuffle\n",
    "import csv\n",
    "import numpy as np\n",
    "from numpy import *\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "import time\n",
    "import h5py\n",
    "from progressbar import ProgressBar\n",
    "import fileinput\n",
    "import pickle\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import SpectralClustering\n",
    "import matplotlib.pyplot as plt\n",
    "import datatable as dt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>sub_category</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://olahraga.kompas.com/read/2011/10/17/09...</td>\n",
       "      <td>Edukasi</td>\n",
       "      <td>Indonesian Corruption Watch menuntut Dinas  Pe...</td>\n",
       "      <td>Edukasi</td>\n",
       "      <td>Icw Datangi Dinas Pendidikan Dki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://travel.kompas.com/read/2009/05/14/2106...</td>\n",
       "      <td>Sains</td>\n",
       "      <td>Untuk mendapatkan populasi orangutan di Kalima...</td>\n",
       "      <td>Sains</td>\n",
       "      <td>Populasi Orangutan Disurvei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://entertainment.kompas.com/read/2008/11/...</td>\n",
       "      <td>Sains</td>\n",
       "      <td>- Direktur Jenderal Sejarah dan Purbakala Dep...</td>\n",
       "      <td>Sains</td>\n",
       "      <td>Kota Majapahit Dibangun Dengan Sistem Jaringan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://entertainment.kompas.com/read/2008/09/...</td>\n",
       "      <td>Oase</td>\n",
       "      <td>--Masyarakat Kristen Indonesia di Yogyakarta, ...</td>\n",
       "      <td>Wisata</td>\n",
       "      <td>Masyarakat Kristen Diy Temui Sultan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://sains.kompas.com/read/2010/05/18/23143...</td>\n",
       "      <td>Liga Italia</td>\n",
       "      <td>- Pelatih Inter Milan, Jose Mourinho, semakin ...</td>\n",
       "      <td>Olahraga</td>\n",
       "      <td>Mourinho Italia Tidak Menghargai Saya</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link sub_category  \\\n",
       "0  https://olahraga.kompas.com/read/2011/10/17/09...      Edukasi   \n",
       "1  https://travel.kompas.com/read/2009/05/14/2106...        Sains   \n",
       "2  https://entertainment.kompas.com/read/2008/11/...        Sains   \n",
       "3  https://entertainment.kompas.com/read/2008/09/...         Oase   \n",
       "4  https://sains.kompas.com/read/2010/05/18/23143...  Liga Italia   \n",
       "\n",
       "                                                text  category  \\\n",
       "0  Indonesian Corruption Watch menuntut Dinas  Pe...   Edukasi   \n",
       "1  Untuk mendapatkan populasi orangutan di Kalima...     Sains   \n",
       "2   - Direktur Jenderal Sejarah dan Purbakala Dep...     Sains   \n",
       "3  --Masyarakat Kristen Indonesia di Yogyakarta, ...    Wisata   \n",
       "4  - Pelatih Inter Milan, Jose Mourinho, semakin ...  Olahraga   \n",
       "\n",
       "                                               title  \n",
       "0                   Icw Datangi Dinas Pendidikan Dki  \n",
       "1                        Populasi Orangutan Disurvei  \n",
       "2  Kota Majapahit Dibangun Dengan Sistem Jaringan...  \n",
       "3                Masyarakat Kristen Diy Temui Sultan  \n",
       "4              Mourinho Italia Tidak Menghargai Saya  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'new_clean_all_data_with_title.csv'\n",
    "# data = pd.read_csv(filename)\n",
    "\n",
    "data = dt.fread(filename)\n",
    "data = data.to_pandas()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test = train_test_split(data, test_size=0.5, random_state=0, \n",
    "#                                stratify=data[['category']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sub category per main category\n",
    "# cat_sub = []\n",
    "# list_category = ['Ekonomi', 'Hiburan','Kesehatan', 'Lifestyle', 'Olahraga', 'Otomotif', 'Pendidikan', 'Properti', 'Sains', 'Teknologi', 'Wisata']\n",
    "# for i in range(11):\n",
    "#     cat_sub.append(data['sub_category'].loc[data['category']==list_category[i]].unique())\n",
    "# cat_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category\n",
      "Edukasi      2582\n",
      "Ekonomi      2582\n",
      "Hiburan      2582\n",
      "Kesehatan    2582\n",
      "Lifestyle    2582\n",
      "Olahraga     2582\n",
      "Otomotif     2582\n",
      "Properti     2582\n",
      "Sains        2582\n",
      "Teknologi    2582\n",
      "Wisata       2582\n",
      "dtype: int64\n",
      "Shape : (28402, 5)\n"
     ]
    }
   ],
   "source": [
    "#Articles per category\n",
    "group = data.groupby('category')\n",
    "print(group.size())\n",
    "print(\"Shape :\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#PreProcessing\n",
    "\n",
    "t0 = time.time()\n",
    "# factory1 = StemmerFactory()\n",
    "# stemmer = factory1.create_stemmer()\n",
    "text = data['text']\n",
    "factory = StopWordRemoverFactory()\n",
    "stopword = factory.create_stop_word_remover()\n",
    "# stemming and stopword removal process\n",
    "output = pd.DataFrame(columns = ['index','text'])\n",
    "print (\"Starting pre-precossing data (stop words removal)! (\",text.shape[0], \"articles)\" )\n",
    "pbar = ProgressBar()\n",
    "i = 0\n",
    "for x in pbar(text):\n",
    "#     temp = stemmer.stem(text[i])\n",
    "    temp = stopword.remove(x)\n",
    "    temp = [[i,temp]]\n",
    "    temp = pd.DataFrame(temp, columns = ['index','text'])\n",
    "    output   = output.append([temp])\n",
    "#     print('.', end='')\n",
    "    i=+1\n",
    "output = output.set_index('index')\n",
    "print(\"Stop words removal is completed!\")\n",
    "print(\"===================================\")\n",
    "print(\"Starting pre-precossing data (punctuations removal)!\")\n",
    "# remove punctuations\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tagged_data = [TaggedDocument(words=tokenizer.tokenize(_d.lower()), tags=[str(c)]) for c, _d in enumerate(output.text)]\n",
    "print (\"Punctuations removal is completed!\")\n",
    "print(\"===================================\")\n",
    "\n",
    "\n",
    "t1= time.time()\n",
    "\n",
    "print(len(output), \"articles have been pre-processed!\")\n",
    "\n",
    "import pickle\n",
    "with open('new_tagged_data.pkl', 'wb') as f:\n",
    "    pickle.dump(tagged_data, f)\n",
    "\n",
    "print(\"execution time: \", t1-t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tagged_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f97c21ad1a4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Load tagged_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtagged_data1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"new_tagged_data.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tagged_data' is not defined"
     ]
    }
   ],
   "source": [
    "#Load tagged_data\n",
    "tagged_data1 = pickle.load( open( \"new_tagged_data.pkl\", \"rb\" ) )\n",
    "len(tagged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check whether there are empty data in tagged_data\n",
    "empty_index = []\n",
    "counter = len(tagged_data)\n",
    "for i in range(counter):\n",
    "    if not tagged_data[i][0]:\n",
    "#         del tagged_data[i]\n",
    "#         print (\"index\", str(i), \"is deleted\")\n",
    "        empty_index.append(i)\n",
    "len(empty_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters for doc2vec model\n",
    "max_epochs = 100\n",
    "vec_size = 20\n",
    "alpha = 0.025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doc2Vec model\n",
    "t0 = time.time()\n",
    "model = Doc2Vec(vector_size=vec_size,\n",
    "                alpha=alpha, \n",
    "                min_alpha=0.00025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "  \n",
    "model.build_vocab(tagged_data)\n",
    "print(\"Model Building\")\n",
    "for epoch in range(max_epochs):\n",
    "    print(\"Epoch \", epoch+1)\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.epochs)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "    \n",
    "\n",
    "model.save(\"d2v.model\")\n",
    "print(\"execution time: \", time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Load model\n",
    "model= Doc2Vec.load(\"d2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents Amount:  28402\n",
      "Vocabulary:  120382\n"
     ]
    }
   ],
   "source": [
    "docs = len(model.docvecs)\n",
    "vocab = len(model.wv.vocab)\n",
    "print(\"Documents Amount: \", docs)\n",
    "print(\"Vocabulary: \", vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/arif/Data1/ARIF/eureka/eurekeEnv/lib/python3.5/site-packages/ipykernel_launcher.py:5: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "#doc2vec metrics\n",
    "npvec = np.zeros((docs,vec_size))\n",
    "\n",
    "i = 0;\n",
    "while model.docvecs[i] != \"\":\n",
    "    npvec[i] = model.docvecs[i]\n",
    "    i = i + 1\n",
    "    if i == docs:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_l1 = metrics.pairwise.cosine_similarity(npvec_try, npvec)\n",
    "# print(\"%d bytes\" % (y_kmeans.size * y_kmeans.itemsize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# npvec_try= []\n",
    "# data_l1 = []\n",
    "# del(data_l1)\n",
    "# del(npvec_try)\n",
    "# del(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract data for NN\n",
    "# np.savetxt(\"X.csv\", npvec, delimiter=\",\")\n",
    "# label = data['category']\n",
    "# label.to_csv('y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spectral CLustering\n",
    "t0 = time.time()\n",
    "npvec\n",
    "data_l1 = metrics.pairwise.cosine_similarity(npvec)\n",
    "label = 11\n",
    "# sc = SpectralClustering(label,affinity='precomputed', n_init=100)\n",
    "y_kmeans = sc.fit_predict(data_l1)\n",
    "# color = ['red','yellow','blue']\n",
    "#Visualising the clusters\n",
    "for i in range(label):\n",
    "    plt.scatter(npvec[y_kmeans == i, 0], npvec[y_kmeans == i, 1], s = 100, label = str(i+1))\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# dump(sc, 'sc_model.joblib')\n",
    "print(\"Execution Time: \", time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/arif/Data1/ARIF/eureka/eurekeEnv/lib/python3.5/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.cluster.spectral module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.cluster. Anything that cannot be imported from sklearn.cluster is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/media/arif/Data1/ARIF/eureka/eurekeEnv/lib/python3.5/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator SpectralClustering from version 0.19.1 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#load model\n",
    "sc = load('sc_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#most common word based on category\n",
    "list_category = ['ekonomi', 'hiburan','kesehatan', 'lifestyle', 'otomotif', 'pendidikan', 'properti', 'sains', 'teknologi', 'wisata', 'sepakbola']\n",
    "category_count = len(list_category)\n",
    "label_count = 0\n",
    "label = 11\n",
    "while label_count != label:\n",
    "    lst = []\n",
    "    sp = np.where(sc.labels_ == label_count)\n",
    "    loopword = 0\n",
    "#     while loopword != vocab:\n",
    "#         word = model.wv.index2word[loopword]\n",
    "    while loopword != category_count:\n",
    "        word = list_category[loopword]\n",
    "        wvec = model[word].reshape(1,-1)\n",
    "        datatest = metrics.pairwise.cosine_similarity(model[sp], wvec)\n",
    "        lst.append(datatest)\n",
    "        loopword = loopword + 1\n",
    "#     dfg = pd.DataFrame(lst).reshape(1,-1)\n",
    "    file = \"mostCommonWord_cluster\" + str(label_count) + \".csv\"\n",
    "    lstnp =  np.asarray(lst)\n",
    "    lstnp =  lstnp.reshape(category_count,lstnp.shape[1])\n",
    "#     lstnp = np.abs(lstnp)\n",
    "#     dfg.to_csv(file)\n",
    "    np.savetxt(file, lstnp, delimiter=\",\")\n",
    "    label_count = label_count + 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[-0.21205947  2.5159874   0.68523484 -1.8653946   0.3666308   1.4941964\n  0.617962   -0.09356293 -0.09324379  0.4228653  -0.5036252  -0.04311667\n  1.5967413   1.0011854  -0.32518235  1.162178   -1.0020175  -0.2915493\n -0.34907264  1.0368581 ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-30deecf6b00b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sepakbola'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/arif/Data1/ARIF/eureka/eurekeEnv/lib/python3.5/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1165\u001b[0m     \u001b[0;31m# to avoid recursive import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1167\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_pairwise_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m     \u001b[0mX_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/arif/Data1/ARIF/eureka/eurekeEnv/lib/python3.5/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[1;32m    142\u001b[0m         Y = check_array(Y, accept_sparse=accept_sparse, dtype=dtype,\n\u001b[1;32m    143\u001b[0m                         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                         estimator=estimator)\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprecomputed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/arif/Data1/ARIF/eureka/eurekeEnv/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    554\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[-0.21205947  2.5159874   0.68523484 -1.8653946   0.3666308   1.4941964\n  0.617962   -0.09356293 -0.09324379  0.4228653  -0.5036252  -0.04311667\n  1.5967413   1.0011854  -0.32518235  1.162178   -1.0020175  -0.2915493\n -0.34907264  1.0368581 ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "metrics.pairwise.cosine_similarity(model[sp], model['sepakbola'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 2237)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstnp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster label\n",
    "cluster_label = []\n",
    "common_words = []\n",
    "for i in range(label):\n",
    "    filename = \"./mostCommonWord_cluster\" + str(i) + \".csv\"\n",
    "    clusterdata = genfromtxt(filename, delimiter=',')\n",
    "    cluster_mean = []\n",
    "    temps = []\n",
    "    for x in range(label):\n",
    "        temp = [np.mean(clusterdata[x]),list_category[x]]\n",
    "        temps.append(temp)\n",
    "        cluster_mean.append(temp[0])\n",
    "    cluster_label.append(list_category[np.argmax(cluster_mean)])\n",
    "    common_words.append(temps)\n",
    "# print('Cluster label : ', cluster_label)\n",
    "for i in range (label):\n",
    "    common_words[i].sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kesehatan',\n",
       " 'teknologi',\n",
       " 'sepakbola',\n",
       " 'lifestyle',\n",
       " 'teknologi',\n",
       " 'properti',\n",
       " 'pendidikan',\n",
       " 'lifestyle',\n",
       " 'sains',\n",
       " 'wisata',\n",
       " 'properti']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.072393</td>\n",
       "      <td>otomotif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.067781</td>\n",
       "      <td>sepakbola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.027262</td>\n",
       "      <td>wisata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.019472</td>\n",
       "      <td>properti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.056399</td>\n",
       "      <td>lifestyle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.111612</td>\n",
       "      <td>hiburan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.117961</td>\n",
       "      <td>teknologi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.171216</td>\n",
       "      <td>sains</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.215492</td>\n",
       "      <td>ekonomi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.274463</td>\n",
       "      <td>pendidikan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.437157</td>\n",
       "      <td>kesehatan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0           1\n",
       "0  -0.072393    otomotif\n",
       "1  -0.067781   sepakbola\n",
       "2  -0.027262      wisata\n",
       "3  -0.019472    properti\n",
       "4   0.056399   lifestyle\n",
       "5   0.111612     hiburan\n",
       "6   0.117961   teknologi\n",
       "7   0.171216       sains\n",
       "8   0.215492     ekonomi\n",
       "9   0.274463  pendidikan\n",
       "10  0.437157   kesehatan"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(common_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-a50242933f73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommon_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(common_words[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_kmeans' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-ca19b325c3ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mdump\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpvec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_kmeans\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cluster\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"UTF-8\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmyfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_kmeans' is not defined"
     ]
    }
   ],
   "source": [
    "#vector all documents per cluster\n",
    "header_row = [\"Doc\"]\n",
    "for i in range (vec_size):\n",
    "    temp = \"vec\" + str(i)\n",
    "    header_row.append(temp)\n",
    "for i in range (label):\n",
    "    dump = []\n",
    "    for x in range(docs):\n",
    "        if sc.labels_[x] == i:\n",
    "            dvec = model[x]\n",
    "            doc = np.array([x])\n",
    "            temp = np.concatenate((doc,dvec),axis=0)       \n",
    "            dump = np.concatenate((dump,temp),axis=0)\n",
    "    a = np.expand_dims(dump, axis=1)\n",
    "    a = a.reshape(len(npvec[y_kmeans == i]), vec_size+1)\n",
    "    file = \"cluster\" + str(i) + \".csv\"\n",
    "    with open(file, 'w', encoding=\"UTF-8\", newline='') as myfile:\n",
    "        wr = csv.writer(myfile)\n",
    "#         wr.writerow((\"Cluster \", label))\n",
    "        wr.writerow((header_row))\n",
    "        wr.writerows(a)\n",
    "    myfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final result\n",
    "predicted_label = []\n",
    "cluster_no = []\n",
    "for i in range (docs):\n",
    "    for x in range (label):\n",
    "        if sc.labels_[i] == x:\n",
    "            predicted_label.append(cluster_label[x])\n",
    "            cluster_no.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top-5\n",
    "top_5 = []\n",
    "for i in range (docs):\n",
    "    for x in range (label):\n",
    "        if sc.labels_[i] == x:\n",
    "            temp = []\n",
    "            for j in range (6, 11):\n",
    "                temp.append(common_words[x][j][1])\n",
    "            top_5.append(temp)\n",
    "#             top_5.append(common_words[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.concat([data['link'],data['category'], pd.DataFrame(cluster_no, columns= ['cluster']), pd.DataFrame(predicted_label, columns= ['predicted_label'])], axis=1)\n",
    "final_data.loc[final_data.category == 'Edukasi', 'category'] = 'Pendidikan'\n",
    "final_data.loc[final_data.category == 'Olahraga', 'category'] = 'Sepakbola'\n",
    "bestresult = np.zeros(docs)\n",
    "for i in range (docs):\n",
    "    if final_data.category[i].lower() == final_data.predicted_label[i]:\n",
    "        bestresult[i]=1\n",
    "final_data = pd.concat([final_data, pd.DataFrame(bestresult, columns=['score'])], axis=1)\n",
    "top_5 = pd.concat([data['link'],data['category'], pd.DataFrame(cluster_no, columns= ['cluster']),pd.DataFrame(top_5, columns = [\"label5\",\"label4\",\"label3\",\"label2\",\"label1\"])], axis=1)\n",
    "top_5.loc[top_5.category == 'Edukasi', 'category'] = 'Pendidikan'\n",
    "top_5.loc[top_5.category == 'Olahraga', 'category'] = 'Sepakbola'\n",
    "final_data.to_csv('final_data.csv')\n",
    "top_5.to_csv('top_5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best 5\n",
    "top5label =  [\"label5\",\"label4\",\"label3\",\"label2\",\"label1\"]\n",
    "top5result = np.zeros(docs)\n",
    "for i in range (docs):\n",
    "    for lab in top5label:\n",
    "        if top_5.category[i].lower() == top_5[lab][i]:\n",
    "            top5result[i]=1\n",
    "finaltop5 = pd.concat([top_5['category'], top_5['cluster'], pd.DataFrame(top5result, columns=['score'])], axis=1)\n",
    "finaltop5.to_csv('final_score_top5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-3f4602325e45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Best 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtop3label\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m[\u001b[0m\u001b[0;34m\"label3\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"label2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"label1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtop3result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlab\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtop3label\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'docs' is not defined"
     ]
    }
   ],
   "source": [
    "#Best 3\n",
    "top3label =  [\"label3\",\"label2\",\"label1\"]\n",
    "top3result = np.zeros(docs)\n",
    "for i in range (docs):\n",
    "    for lab in top3label:\n",
    "        if top_5.category[i].lower() == top_5[lab][i]:\n",
    "            top3result[i]=1\n",
    "finaltop3 = pd.concat([top_5['category'], top_5['cluster'], pd.DataFrame(top3result, columns=['score'])], axis=1)\n",
    "finaltop3.to_csv('final_score_top3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best 1 Accuracy: \", (final_data.loc[final_data.score==1].shape[0]/docs)*100, \"%\")\n",
    "print(\"Best 3 Accuracy: \", (finaltop3.loc[finaltop3.score==1].shape[0]/docs)*100, \"%\")\n",
    "print(\"Best 5 Accuracy: \", (finaltop5.loc[finaltop5.score==1].shape[0]/docs)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(label):\n",
    "    print(\"n_count cluster\", i, \" = \", len(np.where(sc.labels_ == i)[0]), \"Cluster label: \", cluster_label[i])\n",
    "print(list_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = pd.concat([final_data, finaltop3['score'], finaltop5['score']], axis=1)\n",
    "final_result.columns = ['link', 'category', 'cluster', 'predicted_label', 'best1score', 'best3score',\n",
    "       'best5score']\n",
    "final_result.to_csv(\"final_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eurekaEnv",
   "language": "python",
   "name": "eurekaenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
